---
title: 网络丢包的追查
date: 2020-05-30 15:36:47
tags: TCP
---



## 问题背景

最近业务反馈了一个问题：业务进程启动过程很慢，通过pstack查看卡在了我们提供的组件内。这个问题出现在业务的preonline环境，生产环境并没有出现。所以我认为与我们的服务关系不大，但是究竟什么回事，还是需要有个说法。

## 追查过程

### 现象

业务进程启动过程中的pstack状态如下

<!--more-->

{% asset_img  pstack.png Pstack %}

明显卡在了connect调用中。以前遇到过跨机房时connect耗时长，我检查了两个机器的hostname，处于同一机房。

### 找规律

手动重启业务进程，strace去看看系统调用情况，结果发现业务机连接同一个远程机器也是偶尔出现connect超时。

{% asset_img strace1.png Strace %}

上面的截图中向xx.xx.90.196 建立5条连接，但是只有中间的第三次出现connct超时。

有时候服务繁忙负载很高，处理三次握手不及时会出现connect超时，但是一般connect重试之后也就OK了。可以查看服务器状态，当时服务端负载不高。通过netstat -s检查sync丢弃和backlog队列情况：

{% asset_img tcp_state.png TcpState %}

没有出现sync和backlog丢弃的持续增长的。现象

而且只有连接特定几台会出现connect超时，这些connect超时的机器网段一样，看上去是链路上某个硬件出问题丢包导致。

### 抓包

我在业务机器和我们的服务集群上执行tcpdump，抓包看看究竟是三次握手过程中服务端没有返回ack，或者tcp三次握手的syn packet没有被服务器收到

先看抓到的业务机器连接情况

{% asset_img tcp_dump1.png Dump1 %}

业务机器连接x.x.90.205:1315时有一次出现了多时发送Sync Packet的情况(tcpdump中 Flags [S] 表示发送 sync)，当时的本地socket为： x.x.150.74:59357。截图第二段为只看该socket连接情况，明显看出tcp的超时机制，超时间隔为1，2，4，8，16，32，[64], 所以总超时时间为127s。

再看看x.x.90.205 处理sync packet的情况

{% asset_img tcp_dump2.png Dump2 %}

执行`grep x.x.150.74.59357` 发现没有收到客户端发送的连接请求，但是客户端socket x.x.150.74:60660 和x.x.150.74.60662 却是有记录的。

这充分说明链路上是有丢包现象的。

### 解决

丢包的问题一般只能反馈给PE来处理了。我把我的问题向PE说了一遍，PE只是执行ping，traceroute，mtr之后说没有丢包，连通性挺好的。

实际上ping使用的是ICMP协议，mtr默认也使用ICMP的数据包来测试的。查看了手册之后，执行mtr -T, -T参数指定使用sync packet来探测网络丢包情况，结果出现20%的丢包率。

这样PE才认可了丢包的结论。最后PE层次排查，找到了一个转发设备，板上的一块芯片出故障了，会固定的丢弃带有sync的5元组。把这块板隔离之后就没有出现超时的情况了。

## 总结

这次追查过程比较坎坷，断断续续持续了3天，还好最终查到了问题。这次查的问题有特殊性，如果是普通丢包，ping就能看出来。这也是后来跟PE掰扯的根源，我告诉PE建立连接阶段丢包，但是PE才不管你三次握手还是四次握手，PE的逻辑是长时间ping没有丢包就是没有丢包。

